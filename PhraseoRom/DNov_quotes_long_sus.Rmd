---
title: "DNov quotes vs. long suspensions"
author: "Viola Wiegand, Anthony Hennessey and Michaela Mahlberg"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: markdown_github
    toc: yes
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 1000)
```
# Data
Libraries
```{r libraries}
library(CorporaCoCo)
library(data.table)
library(stringi)
```

## Get the corp\_text objects

```{r import corp_text objects}
quotes_files <- list.files("./api-output-sem-tagged/DNov/quote_corp_text_objects/", full.names = TRUE)
quotes_objects <- lapply(quotes_files, readRDS)
quotes_text <- corp_text_rbindlist(quotes_objects)

long_sus_files <- list.files("./api-output-sem-tagged/DNov/longsus_corp_text_objects/", full.names = TRUE)
long_sus_objects <- lapply(long_sus_files, readRDS)
long_sus_text <- corp_text_rbindlist(long_sus_objects)
```

## Create a data.table where each column is a set of types for the tokens
The loaded `corp_text` objects have the full semantic tags as the types

semantic_tags
:  keep whole of first tag, but not multipart or lowercase bits e.g. `A1.1.1+`. Remove [+-]? to not keep the plus or minus.

lc_tokens
:  lowercased tokens

```{r create_type_lookup}
quotes_type_store <- data.frame(
    lc_tokens = tolower(quotes_text$tokens$token),
    semantic_tags = stri_extract_first(quotes_text$tokens$type, regex = "^([A-Z]\\d+(?:\\.\\d+)*[+-]?)"),
    stringsAsFactors = FALSE
)
long_sus_type_store <- data.frame(
    lc_tokens = tolower(long_sus_text$tokens$token),
    semantic_tags = stri_extract_first(long_sus_text$tokens$type, regex = "^([A-Z]\\d+(?:\\.\\d+)*[+-]?)"),
    stringsAsFactors = FALSE
)
```

## Corpus size
Quotes
```{r corpus size quotes}
#Count tokens without the co-occurrence barriers (i.e. ssubset boundaries) _b_ (NA)
sum(quotes_text$tokens$token!="_b_", na.rm = TRUE)
```

Long suspensions
```{r corpus size long_sus}
#Count tokens without the co-occurrence barriers (i.e. ssubset boundaries) _b_ (NA)
sum(long_sus_text$tokens$token!="_b_", na.rm = TRUE)
```


# Unmatched tokens
The Z99 tag contains "unmatched" tokens. These can be looked up like this for the quotes (but take up a lot of space):
```{r Z99 quotes}
#a <- corp_type_lookup(quotes_text)
#a[type == "Z99"]$tokens
```

And like this for the long suspensions
```{r Z99 long_sus}
#b <- corp_type_lookup(long_sus_text)
# b[type == "Z99"]$tokens
```

# Analysis: 1. Comparing manually chosen, specific body part terms + "lexical" collocates in DNov quotes vs. long suspensions

```{r comparison of specific body parts}
quotes_text$tokens$type <- with(quotes_type_store, lc_tokens)
quotes_text$tokens$type[is.na(quotes_type_store$semantic_tags)] <- NA  # reintroduce the cooccurence barriers
long_sus_text$tokens$type <- with(long_sus_type_store, lc_tokens)
long_sus_text$tokens$type[is.na(long_sus_type_store$semantic_tags)] <- NA  # reintroduce the cooccurence barriers

head(quotes_text$tokens, 20)
head(long_sus_text$tokens, 20)

# the set of nodes and collocates we are interested in
nodes <- c('eye', 'eyes', 'forehead', 'hand', 'hands', 'head', 'shoulder')
nodes

#co-occurrences
quotes_surface <- corp_surface(quotes_text, span = "5LR", nodes = nodes)
long_sus_surface <- corp_surface(long_sus_text, span = "5LR", nodes = nodes)

# compare quotes vs. long suspensions
results <- corp_coco(quotes_surface, long_sus_surface, nodes = nodes)
```

## Figure 1: Plot of specific, manually chosen body part terms +"lexical" collocates in DNov quotes vs. long suspensions

```{r body_parts_DNov_quotes_vs_long_sus_lexical, fig.width = 10, fig.height = 35, dpi=300}
plot(results)
```

### Concordances: Quotes
```{r body part concordances for lexical collocates quotes}
# your
y <- corp_concordance(quotes_surface, nodes = nodes, collocates= c("your"), context = 0)
y

# you
y <- corp_concordance(quotes_surface, nodes = nodes, collocates= c("you"), context = 0)
y
```

### Concordances: Long suspensions
```{r body part concordances for lexical collocates long suspensions}
# returned
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("returned"), context = 0)
y

# nodding
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("nodding"), context = 0)
y

# replied
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("replied"), context = 0)
y
```

# Analysis: 2. Comparing B1 + all "lexical" collocates in DNov quotes vs. long suspensions

For this part, only the tag B1 is kept, all others are replaced by lower case tokens. This means we are working with a version of the corpus where only B1 semantic tags are present, whereas everything else is shown as its original token form (but in lower case). This allows us to see co-occurrences such as B1 + "rubbing". If B1 co-occurred more frequently with itself in one of the corpora, the co-occurrence pair B1 + B1 would also show up (as all realizations of the tag B1 are disaplyed as "B1" rather than their tokens at this stage).

```{r keep only B1}
quotes_text$tokens$type <- with(quotes_type_store, ifelse(grepl("^B1", semantic_tags), semantic_tags, lc_tokens))
quotes_text$tokens$type[is.na(quotes_type_store$semantic_tags)] <- NA  # reintroduce the cooccurence barriers
long_sus_text$tokens$type <- with(long_sus_type_store, ifelse(grepl("^B1", semantic_tags), semantic_tags, lc_tokens))
long_sus_text$tokens$type[is.na(long_sus_type_store$semantic_tags)] <- NA  # reintroduce the cooccurence barriers
```

## Number of (lexical) types & tokens in B1 & examples
### Quotes
```{r type mapping quotes}
a <- corp_type_lookup(quotes_text)

# Number of tokens tagged as B1 in quotes
nrow(quotes_text$tokens[type=="B1"])

# Top 20 frequency of (lexical) types tagged as B1
freq_list <- (as.data.frame(sort(table(quotes_text$tokens[type=="B1"]$token), decreasing = TRUE)))
head(freq_list, 20)
              
# All (lexical) types tagged as B1 in quotes
unique(tolower(a[type == "B1"]$tokens))
```

### Long suspensions
```{r type mapping long_sus}
b <- corp_type_lookup(long_sus_text)

# Number of tokens tagged as B1 in long suspensions
nrow(long_sus_text$tokens[type=="B1"])

# Top 20 frequency of (lexical) types tagged as B1
freq_list <- (as.data.frame(sort(table(long_sus_text$tokens[type=="B1"]$token), decreasing = TRUE)))
head(freq_list, 20)

# All (lexical) types tagged as B1 in long suspensions
unique(tolower(b[type == "B1"]$tokens))
```


``` {r comparison of B1}
# the set of nodes and collocates we are interested in
nodes <- unique(grep("^B1", c(quotes_text$tokens$type, long_sus_text$tokens$type), value = TRUE))
nodes

#co-occurrences
quotes_surface <- corp_surface(quotes_text, span = "5LR", nodes = nodes)
long_sus_surface <- corp_surface(long_sus_text, span = "5LR", nodes = nodes)

# compare
results <- corp_coco(quotes_surface, long_sus_surface, nodes = nodes)
```

## Concordances of "lexical" collocates

### Concordances: Quotes
```{r B1 concordances for lexical collocates quotes}
# "to" as the realisation of B1
y <- corp_concordance(quotes_surface, nodes = nodes, context = 0)
# In order to subset by the node "to" as a B1 realisation and sort by the first word on the right, in this case sort by "R1", not "R1_type" (the type is the semantic tag and in this case it will be B1 for all of the R1s, because "to" is part of multiword sequences that are tagged for B1 as a unit, with "to" in the middle)
subset(y[order(R1),], N=="to")

```

### Concordances: Long suspensions
```{r B1 concordances for lexical collocates long suspensions}
# returned
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("returned"), context = 0)
y

# replied
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("replied"), context = 0)
y

# cried
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("cried"), context = 0)
y

# rubbing
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("rubbing"), context = 0)
y

# shaking
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("shaking"), context = 0)
y

# nodding
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("shaking"), context = 0)
y

# laying
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("laying"), context = 0)
y

# glancing
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("glancing"), context = 0)
y

# surveying
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("surveying"), context = 0)
y

# casting
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("casting"), context = 0)
y

# striking
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("striking"), context = 0)
y

# dropping
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("dropping"), context = 0)
y

# opening
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("opening"), context = 0)
y

# glistening
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("glistening"), context = 0)
y

# fixing
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("fixing"), context = 0)
y

# rolling
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("rolling"), context = 0)
y

# turning
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("turning"), context = 0)
y

# shading
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("shading"), context = 0)
y

# shutting
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("shutting"), context = 0)
y

# lifting
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("lifting"), context = 0)
y

# raising
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("raising"), context = 0)
y

# drawing
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("drawing"), context = 0)
y
```

### Figure 2: Plot of B1 +"lexical" collocates in DNov quotes vs. long suspensions

```{r B1_DNov_quotes_vs_long_sus_lexical, fig.width = 10, fig.height = 70, dpi=300}
plot(results)
```

# Analysis: 3. Comparing B1 + all semantic types in DNov quotes vs. long suspensions

```{r B1 vs all semantic types}
quotes_text$tokens$type <- with(quotes_type_store, semantic_tags)
long_sus_text$tokens$type <- with(long_sus_type_store, semantic_tags)

head(quotes_text$tokens, 20)
head(long_sus_text$tokens, 20)

# the set of nodes and collocates we are interested in
nodes <- unique(grep("^B1", c(quotes_text$tokens$type, long_sus_text$tokens$type), value = TRUE))
nodes

quotes_surface <- corp_surface(quotes_text, span = "5LR", nodes = nodes)
long_sus_surface <- corp_surface(long_sus_text, span = "5LR", nodes = nodes)

# compare
results <- corp_coco(quotes_surface, long_sus_surface, nodes = nodes)
```

## Figure 3: Plot of B1 + "semantic tag collocates" in DNov quotes vs. long suspensions 
```{r B1_DNov_quotes_vs_long_sus_semantic, fig.width = 10, fig.height = 20, dpi=300}
plot(results)
```

## Concordances of semantic tag collocates

The concordances below have been picked relatively spontaneously from among the high effect size differences and the unique results. We can easily add other concordances.

### Concordances: Long suspensions
```{r B1 concordances for semantic collocates long suspensions}
# "I3.2" - "Work and employment: Professionalism""
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("I3.2"), context = 0)
y

# "Q2.1" - "Speech etc: Communicative"
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("Q2.1"), context = 0)
y

# "X5.1+" - "Attention"
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("X5.1+"), context = 0)
y

# "N3.8" - "Measurement: Speed"
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("N3.8"), context = 0)
y

# "O1.2-" - "Substances and materials generally: Liquid -"
y <- corp_concordance(long_sus_surface, nodes = nodes, collocates= c("O1.2-"), context = 0)
y
```


# How this document was generated
This document is written in [rmarkdown](https://cran.r-project.org/package=rmarkdown).
The intension is to use [knitr](https://cran.r-project.org/package=knitr) to generate either `html` or `pdf` versions of the document.
The advantage of `rmarkdown` is that `R` code examples can be embedded in the document -- the code samples being executed and the results generated when the document is 'compiled'.
Details of the citation syntax can be found in the [pandoc documentation](http://pandoc.org/MANUAL.html#citations).
Details of equation numbering can be found in the [MathJax documentation](http://mathjax.readthedocs.io/en/latest/tex.html#automatic-equation-numbering).
Combining code with documentation is a form of [literate programming](https://www-cs-faculty.stanford.edu/~knuth/lp.html).

In a linux environment the `html` document can be generated on the command liner like this:
```bash
Rdev -e "rmarkdown::render('DNov_quotes_long_sus.Rmd')"
```
